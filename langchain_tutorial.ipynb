{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI\n",
    "#from langchain_anthropic import ChatAnthropic\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=HuggingFaceEndpoint(repo_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",task=\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.invoke(\"what is capital of USA?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_model=ChatGoogleGenerativeAI(model='gemini-1.5-pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-pro\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-pro\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-pro\"\n",
      "  }\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 16\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "ename": "ResourceExhausted",
     "evalue": "429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-1.5-pro\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-1.5-pro\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-1.5-pro\"\n  }\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 14\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgemini_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhat is capital of USA?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\agentic_base_env\\lib\\site-packages\\langchain_google_genai\\chat_models.py:1255\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI.invoke\u001b[1;34m(self, input, config, code_execution, stop, **kwargs)\u001b[0m\n\u001b[0;32m   1250\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1251\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1252\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTools are already defined.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_execution tool can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be defined\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1253\u001b[0m         )\n\u001b[1;32m-> 1255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\agentic_base_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:372\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    368\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    369\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 372\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    373\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    374\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    375\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    376\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    377\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    378\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    379\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    380\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    381\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    382\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\agentic_base_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:957\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    950\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    955\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    956\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 957\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\agentic_base_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:776\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[0;32m    774\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    775\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 776\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    777\u001b[0m                 m,\n\u001b[0;32m    778\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    779\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    780\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    781\u001b[0m             )\n\u001b[0;32m    782\u001b[0m         )\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\agentic_base_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1022\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1022\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m   1023\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1024\u001b[0m     )\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1026\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\agentic_base_env\\lib\\site-packages\\langchain_google_genai\\chat_models.py:1342\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI._generate\u001b[1;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate\u001b[39m(\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1318\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1329\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1330\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m   1331\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(\n\u001b[0;32m   1332\u001b[0m         messages,\n\u001b[0;32m   1333\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1340\u001b[0m         tool_choice\u001b[38;5;241m=\u001b[39mtool_choice,\n\u001b[0;32m   1341\u001b[0m     )\n\u001b[1;32m-> 1342\u001b[0m     response: GenerateContentResponse \u001b[38;5;241m=\u001b[39m _chat_with_retry(\n\u001b[0;32m   1343\u001b[0m         request\u001b[38;5;241m=\u001b[39mrequest,\n\u001b[0;32m   1344\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1345\u001b[0m         generation_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mgenerate_content,\n\u001b[0;32m   1346\u001b[0m         metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_metadata,\n\u001b[0;32m   1347\u001b[0m     )\n\u001b[0;32m   1348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\agentic_base_env\\lib\\site-packages\\langchain_google_genai\\chat_models.py:210\u001b[0m, in \u001b[0;36m_chat_with_retry\u001b[1;34m(generation_method, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m--> 210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _chat_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\agentic_base_env\\lib\\site-packages\\tenacity\\__init__.py:338\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    336\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    337\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m copy(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\agentic_base_env\\lib\\site-packages\\tenacity\\__init__.py:477\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 477\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\agentic_base_env\\lib\\site-packages\\tenacity\\__init__.py:378\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    376\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[1;32m--> 378\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\agentic_base_env\\lib\\site-packages\\tenacity\\__init__.py:420\u001b[0m, in \u001b[0;36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[1;34m(rs)\u001b[0m\n\u001b[0;32m    418\u001b[0m retry_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_error_cls(fut)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\agentic_base_env\\lib\\site-packages\\tenacity\\__init__.py:187\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mNoReturn:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mfailed:\n\u001b[1;32m--> 187\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_attempt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\agentic_base_env\\lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\agentic_base_env\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\agentic_base_env\\lib\\site-packages\\tenacity\\__init__.py:480\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 480\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    482\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\agentic_base_env\\lib\\site-packages\\langchain_google_genai\\chat_models.py:208\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    206\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\agentic_base_env\\lib\\site-packages\\langchain_google_genai\\chat_models.py:192\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_chat_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_method(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mFailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\agentic_base_env\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:868\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m    867\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\agentic_base_env\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\agentic_base_env\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    291\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    293\u001b[0m )\n\u001b[1;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\agentic_base_env\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m     next_sleep \u001b[38;5;241m=\u001b[39m \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(next_sleep)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\agentic_base_env\\lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[0;32m    209\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    210\u001b[0m         error_list,\n\u001b[0;32m    211\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[0;32m    212\u001b[0m         original_timeout,\n\u001b[0;32m    213\u001b[0m     )\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\agentic_base_env\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    149\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\agentic_base_env\\lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout\n\u001b[0;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\agentic_base_env\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mResourceExhausted\u001b[0m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-1.5-pro\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-1.5-pro\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-1.5-pro\"\n  }\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 14\n}\n]"
     ]
    }
   ],
   "source": [
    "gemini_model.invoke(\"what is capital of USA?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_model=ChatGroq(model=\"deepseek-r1-distill-llama-70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<think>\\nOkay, so I need to figure out what the capital of the USA is. Hmm, I'm not entirely sure, but I think it's a city I've heard of before. Let me try to remember. I know that the USA has states, and each state has its own capital, but the country as a whole has a federal capital.\\n\\nWait, I remember learning something about this in school. I think it's Washington, but is that the city or the state? Oh right, Washington is a state too. So the capital city must be in that state. Maybe it's called Washington D.C.? Yeah, that sounds familiar. D.C. stands for District of Columbia, right? So the capital is Washington, D.C.\\n\\nI think the reason it's not just called Washington is because it's a special district, not part of any state. That makes sense because it's the seat of the federal government. So the capital isn't in any state but is its own entity. I guess that's to ensure it's neutral and not favor any particular state.\\n\\nLet me double-check. I remember visiting monuments like the White House and the Capitol Building. Those are in Washington, D.C., so that must be where the government is centered. So yes, the capital of the USA is Washington, D.C., not just Washington. I think that's the correct answer.\\n</think>\\n\\nThe capital of the United States of America is Washington, D.C. (District of Columbia). It is a federal district serving as the permanent capital and is not part of any state, ensuring neutrality in the federal government.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 327, 'prompt_tokens': 8, 'total_tokens': 335, 'completion_time': 1.5981963750000001, 'prompt_time': 0.000333036, 'queue_time': 0.20382585, 'total_time': 1.598529411}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'finish_reason': 'stop', 'logprobs': None}, id='run--6d134383-c5bd-4f0f-bfbe-0b738a89cd11-0', usage_metadata={'input_tokens': 8, 'output_tokens': 327, 'total_tokens': 335})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groq_model.invoke(\"what is capital of USA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_model=ChatOpenAI(model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of the USA is Washington D.C.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 12, 'total_tokens': 22, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-BleioRKbO4CnjvnQSgSd61twXln55', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--15539f76-1fb1-4d5b-8634-d230c28f60b2-0', usage_metadata={'input_tokens': 12, 'output_tokens': 10, 'total_tokens': 22, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_model.invoke(\"what is capital of USA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\agentic_base_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\subhr\\.cache\\huggingface\\hub\\models--TinyLlama--TinyLlama-1.1B-Chat-v1.0. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
    "    task='text-generation',\n",
    "    pipeline_kwargs=dict(\n",
    "        temperature=0.5,\n",
    "        max_new_tokens=100\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_embedding=OpenAIEmbeddings(model='text-embedding-3-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=openai_embedding.embed_query(\"India is a growing country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_embedding_small=OpenAIEmbeddings(model='text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2=openai_embedding_small.embed_query(\"India is a growing country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_embedding_64=OpenAIEmbeddings(model='text-embedding-3-large',dimensions=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3=openai_embedding_64.embed_query(\"India is a growing country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[\"what is a capital of USA\",\n",
    "           \"who is a president of usa\",\n",
    "           \"who is a prime minister of india\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=openai_embedding.embed_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0723918005824089,\n",
       " 0.15564556419849396,\n",
       " -0.009400391951203346,\n",
       " 0.11175045371055603,\n",
       " -0.1681687831878662,\n",
       " -0.05012477561831474,\n",
       " 0.09993007779121399,\n",
       " 0.11871489137411118,\n",
       " -0.20228812098503113,\n",
       " -0.019200120121240616,\n",
       " -0.029007835313677788,\n",
       " 0.12012055516242981,\n",
       " -0.19321519136428833,\n",
       " -0.3414490520954132,\n",
       " 0.0063095237128436565,\n",
       " -0.025014465674757957,\n",
       " -0.2385798692703247,\n",
       " 0.2573646903038025,\n",
       " 0.14107775688171387,\n",
       " 0.1100253164768219,\n",
       " 0.11730922013521194,\n",
       " 0.05446955934166908,\n",
       " -0.08012296259403229,\n",
       " 0.014775467105209827,\n",
       " -0.013401747681200504,\n",
       " 0.02817721478641033,\n",
       " 0.08172031491994858,\n",
       " -0.004780063405632973,\n",
       " 0.09571307897567749,\n",
       " 0.08459553867578506,\n",
       " 0.1895093470811844,\n",
       " 0.07584207504987717,\n",
       " 0.19462086260318756,\n",
       " -0.13027969002723694,\n",
       " 0.0063454643823206425,\n",
       " 0.08312597870826721,\n",
       " 0.04485352709889412,\n",
       " 0.010718204081058502,\n",
       " -0.05325557664036751,\n",
       " 0.09890777617692947,\n",
       " 0.07251959294080734,\n",
       " 0.020046714693307877,\n",
       " -0.15347318351268768,\n",
       " 0.21059434115886688,\n",
       " 0.3054128885269165,\n",
       " -0.06309524178504944,\n",
       " 0.1791585236787796,\n",
       " -0.17762507498264313,\n",
       " 0.06862206012010574,\n",
       " 0.11251717805862427,\n",
       " -0.08382881432771683,\n",
       " -0.03712236136198044,\n",
       " -0.06938879191875458,\n",
       " -0.07156118005514145,\n",
       " -0.0470578670501709,\n",
       " -0.04223387688398361,\n",
       " -0.06894153356552124,\n",
       " -0.028943942859768867,\n",
       " -0.22413983941078186,\n",
       " -0.06843037903308868,\n",
       " 0.030125979334115982,\n",
       " -0.12184569239616394,\n",
       " 0.007403707131743431,\n",
       " 0.14388908445835114]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[\"what is a capital of USA\",\n",
    "           \"who is a president of usa\",\n",
    "           \"who is a prime minister of india\"]\n",
    "\n",
    "my_query=\"narendra modi is indian prime minister\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding=openai_embedding.embed_query(my_query)\n",
    "document_embedding=openai_embedding.embed_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.0723918005824089,\n",
       "  0.15564556419849396,\n",
       "  -0.009400391951203346,\n",
       "  0.11175045371055603,\n",
       "  -0.1681687831878662,\n",
       "  -0.05012477561831474,\n",
       "  0.09993007779121399,\n",
       "  0.11871489137411118,\n",
       "  -0.20228812098503113,\n",
       "  -0.019200120121240616,\n",
       "  -0.029007835313677788,\n",
       "  0.12012055516242981,\n",
       "  -0.19321519136428833,\n",
       "  -0.3414490520954132,\n",
       "  0.0063095237128436565,\n",
       "  -0.025014465674757957,\n",
       "  -0.2385798692703247,\n",
       "  0.2573646903038025,\n",
       "  0.14107775688171387,\n",
       "  0.1100253164768219,\n",
       "  0.11730922013521194,\n",
       "  0.05446955934166908,\n",
       "  -0.08012296259403229,\n",
       "  0.014775467105209827,\n",
       "  -0.013401747681200504,\n",
       "  0.02817721478641033,\n",
       "  0.08172031491994858,\n",
       "  -0.004780063405632973,\n",
       "  0.09571307897567749,\n",
       "  0.08459553867578506,\n",
       "  0.1895093470811844,\n",
       "  0.07584207504987717,\n",
       "  0.19462086260318756,\n",
       "  -0.13027969002723694,\n",
       "  0.0063454643823206425,\n",
       "  0.08312597870826721,\n",
       "  0.04485352709889412,\n",
       "  0.010718204081058502,\n",
       "  -0.05325557664036751,\n",
       "  0.09890777617692947,\n",
       "  0.07251959294080734,\n",
       "  0.020046714693307877,\n",
       "  -0.15347318351268768,\n",
       "  0.21059434115886688,\n",
       "  0.3054128885269165,\n",
       "  -0.06309524178504944,\n",
       "  0.1791585236787796,\n",
       "  -0.17762507498264313,\n",
       "  0.06862206012010574,\n",
       "  0.11251717805862427,\n",
       "  -0.08382881432771683,\n",
       "  -0.03712236136198044,\n",
       "  -0.06938879191875458,\n",
       "  -0.07156118005514145,\n",
       "  -0.0470578670501709,\n",
       "  -0.04223387688398361,\n",
       "  -0.06894153356552124,\n",
       "  -0.028943942859768867,\n",
       "  -0.22413983941078186,\n",
       "  -0.06843037903308868,\n",
       "  0.030125979334115982,\n",
       "  -0.12184569239616394,\n",
       "  0.007403707131743431,\n",
       "  0.14388908445835114],\n",
       " [-0.11559119820594788,\n",
       "  0.14581620693206787,\n",
       "  -0.056344516575336456,\n",
       "  0.05963599681854248,\n",
       "  0.0005394558538682759,\n",
       "  -0.11417550593614578,\n",
       "  -0.025588620454072952,\n",
       "  0.15629231929779053,\n",
       "  -0.019235705956816673,\n",
       "  0.05039861425757408,\n",
       "  0.02365974150598049,\n",
       "  0.11573276668787003,\n",
       "  -0.05266371741890907,\n",
       "  -0.19182617962360382,\n",
       "  -0.24703814089298248,\n",
       "  0.02967642992734909,\n",
       "  -0.26445114612579346,\n",
       "  0.1155204176902771,\n",
       "  -0.1398703008890152,\n",
       "  0.032100800424814224,\n",
       "  -0.14220619201660156,\n",
       "  0.02158929407596588,\n",
       "  -0.2955963611602783,\n",
       "  -0.06894417107105255,\n",
       "  0.16662687063217163,\n",
       "  0.03765739127993584,\n",
       "  -0.005839726887643337,\n",
       "  -0.11254746466875076,\n",
       "  -0.0021014169324189425,\n",
       "  0.03445438668131828,\n",
       "  -0.14779818058013916,\n",
       "  0.20428426563739777,\n",
       "  0.06689141690731049,\n",
       "  0.08062362670898438,\n",
       "  0.13640186190605164,\n",
       "  0.054220978170633316,\n",
       "  -0.037445034831762314,\n",
       "  -0.1756872981786728,\n",
       "  -0.08203931152820587,\n",
       "  0.019589629024267197,\n",
       "  -0.10624763369560242,\n",
       "  0.11509570479393005,\n",
       "  -0.1487891674041748,\n",
       "  -0.1272706538438797,\n",
       "  0.3904476761817932,\n",
       "  0.08805600553750992,\n",
       "  0.12295279651880264,\n",
       "  0.005313266534358263,\n",
       "  0.028260739520192146,\n",
       "  0.17441317439079285,\n",
       "  -0.09032110869884491,\n",
       "  0.03199462592601776,\n",
       "  0.014484291896224022,\n",
       "  0.08076519519090652,\n",
       "  -0.05071714147925377,\n",
       "  0.0808359757065773,\n",
       "  -0.04965537413954735,\n",
       "  -0.12804928421974182,\n",
       "  -0.06108708307147026,\n",
       "  -0.1398703008890152,\n",
       "  0.13753440976142883,\n",
       "  -0.019288795068860054,\n",
       "  0.006671445444226265,\n",
       "  0.22580277919769287],\n",
       " [-0.1951868087053299,\n",
       "  0.08953013271093369,\n",
       "  0.07249994575977325,\n",
       "  -2.545567895140266e-06,\n",
       "  -0.003054138505831361,\n",
       "  -0.17266525328159332,\n",
       "  -0.02417244017124176,\n",
       "  0.04827537015080452,\n",
       "  -0.16321176290512085,\n",
       "  0.07514136284589767,\n",
       "  -0.005695555359125137,\n",
       "  0.056929487735033035,\n",
       "  0.03446006402373314,\n",
       "  -0.12067104876041412,\n",
       "  -0.13478177785873413,\n",
       "  0.08980818092823029,\n",
       "  -0.04201938211917877,\n",
       "  -0.04740648344159126,\n",
       "  -0.18893082439899445,\n",
       "  -0.0661049336194992,\n",
       "  -0.04733697324991226,\n",
       "  -0.18531624972820282,\n",
       "  -0.1186552345752716,\n",
       "  -0.0910593718290329,\n",
       "  -0.0002713914727792144,\n",
       "  0.12213078141212463,\n",
       "  0.05894530564546585,\n",
       "  -0.15320219099521637,\n",
       "  0.03934321179986,\n",
       "  0.20005257427692413,\n",
       "  -0.03247900307178497,\n",
       "  0.12553681433200836,\n",
       "  0.2100621610879898,\n",
       "  0.022903865203261375,\n",
       "  0.06683479994535446,\n",
       "  -0.011139133013784885,\n",
       "  -0.03788347914814949,\n",
       "  -0.06794697791337967,\n",
       "  -0.09828851372003555,\n",
       "  0.00632984284311533,\n",
       "  -0.2228521853685379,\n",
       "  -0.064853735268116,\n",
       "  -0.16682633757591248,\n",
       "  0.035832908004522324,\n",
       "  0.1304025799036026,\n",
       "  0.16432394087314606,\n",
       "  0.047580260783433914,\n",
       "  -0.06947621703147888,\n",
       "  0.10565667599439621,\n",
       "  0.030428428202867508,\n",
       "  0.1836479902267456,\n",
       "  0.09523003548383713,\n",
       "  -0.052967362105846405,\n",
       "  0.23758850991725922,\n",
       "  -0.21242552995681763,\n",
       "  0.12372953444719315,\n",
       "  -0.016500167548656464,\n",
       "  -0.05258505046367645,\n",
       "  0.12094909697771072,\n",
       "  -0.3000093698501587,\n",
       "  0.0519942082464695,\n",
       "  -0.10621276497840881,\n",
       "  -0.22966425120830536,\n",
       "  0.29500457644462585]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=cosine_similarity([query_embedding],document_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13453207, 0.38540264, 0.69040144]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Complete_Content\\GENERATIVEAI\\NEW_E2E_COURSE\\genai_bootcamp\\env\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sunny\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "huggingface_embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "google_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROMPTS or PROMPT_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage,HumanMessage,AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "illegal target for annotation (1136302640.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[47], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    Human_Message or User_Message: \"can you suggest me a best medicine for fever?\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m illegal target for annotation\n"
     ]
    }
   ],
   "source": [
    "My_Model:\"GPT\"\n",
    "System_Message: \"You are healthcare chatbot.\"\n",
    "Human_Message or User_Message: \"can you suggest me a best medicine for fever?\"\n",
    "AI_Message or Model_Generated_Message: \"paracetamol/DOLO650 is a best medicine for fever.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessage(content='you are a funny bot means whatever you answer, you answer in the funny way', additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SystemMessage(content=\"you are a funny bot means whatever you answer, you answer in the funny way\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content='who is your best friend', additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HumanMessage(content=\"who is your best friend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[SystemMessage(content=\"you are a funny bot means whatever you answer, you answer in the funny way\"),\n",
    "          HumanMessage(content=\"who is your best friend\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages2=[SystemMessage(content=\"you are angery young man, you answer everything in rude way\"),\n",
    "          HumanMessage(content=\"who is your best friend\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My best friend is Google. He knows everything. In fact, sometimes, I think he knows too much. He could probably tell you what you had for breakfast 3 years ago on a Tuesday!'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_model.invoke(messages).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't need a best friend. They're just someone else who would inevitably let you down. Id rather stick to being alone. Its less complicated.\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_model.invoke(messages2).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages3=[SystemMessage(content=\"you are very helpful assistance you answer everything in detail\"),\n",
    "          HumanMessage(content=\"tell me the role of langchain in AI devlopment\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages3.append(AIMessage(openai_model.invoke(messages3).content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='you are very helpful assistance you answer everything in detail', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='tell me the role of langchain in AI devlopment', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Langchain is a term that does not seem to be directly related to AI development based on existing and recognized IT vocabulary. If you are referring to \"Language Models\" or a certain company or technology called \"Langchain\", it may be best to clarify for a more accurate response.\\n\\nIf you\\'re speaking about Language Models in AI development, they play a crucial role. A language model is essentially a model that has been trained to recognize, understand, and generate text that is human-readable and meaningful. They are utilized in many aspects of AI, including speech recognition, machine translation, part of speech tagging, sentiment analysis, and more. Large language models like GPT-3, BERT, and T5 have been making significant strides in the field, providing more human-like text generation and understanding for various applications in the AI landscape.\\n\\nPlease provide more details if you were referring to something else.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a chabtbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [\n",
    "    SystemMessage(content=\"you are a helpful assistant\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Generated Answer: Yes, that's correct! How may I assist you today?\n",
      "AI Generated Answer: Of course! I'm here to help. Please tell me what you need assistance with.\n",
      "AI Generated Answer: Absolutely! Feel free to ask anything you need help with.\n",
      "AI Generated Answer: Yes, indeed! How may I assist you today?\n",
      "AI Generated Answer: Of course! I'm here to help. What do you need assistance with?\n",
      "AI Generated Answer: Yes! How may I assist you today?\n",
      "AI Generated Answer: Yes, I'm here to help. What can I assist you with today?\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input=input(\"user_input: \")\n",
    "    chat_history.append(HumanMessage(content=user_input))\n",
    "    if user_input==\"exit\":\n",
    "        break\n",
    "    result=openai_model.invoke(chat_history)\n",
    "    chat_history.append(AIMessage(result.content))\n",
    "    print(\"AI Generated Answer:\", result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='you are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='what is a capital of india?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of India is New Delhi.', additional_kwargs={}, response_metadata={}), HumanMessage(content='who is a prime minister of india?', additional_kwargs={}, response_metadata={}), AIMessage(content='The Prime Minister of India is Narendra Modi, as of my knowledge update in October 2021.\\n', additional_kwargs={}, response_metadata={}), HumanMessage(content='what is a age of him?', additional_kwargs={}, response_metadata={}), AIMessage(content='Narendra Modi was born on September 17, 1950. So, as of October 2021, he is 71 years old.', additional_kwargs={}, response_metadata={}), HumanMessage(content='what is a age of him?', additional_kwargs={}, response_metadata={}), AIMessage(content='Narendra Modi was born on September 17, 1950. As of 2022, he is 72 years old. Please note that the information is based on the current year, so you may need to adjust his age based on the current year.', additional_kwargs={}, response_metadata={}), HumanMessage(content='so what will a age in 2030?', additional_kwargs={}, response_metadata={}), AIMessage(content='If Narendra Modi was born in 1950, then in 2030, he will be 80 years old.', additional_kwargs={}, response_metadata={}), HumanMessage(content='exit', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=PromptTemplate(\n",
    "    template=\"can you say hello to {name} in 5 different language\",\n",
    "    input_variables=['name']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='can you say hello to {name} in 5 different language')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='can you say hello to {name} in 5 different language')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template.get_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='can you say hello to sunny in 5 different language')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template.invoke({\"name\":\"sunny\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='can you say hello to krish in 5 different language')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template.invoke({\"name\":\"krish\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=template.invoke({\"name\":\"virat kholi\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. English: Hello Virat Kholi.\\n2. Hindi:    (Namaste Virat Kholi).\\n3. Spanish: Hola Virat Kholi.\\n4. French: Bonjour Virat Kholi.\\n5. Italian: Ciao Virat Kholi.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_model.invoke(prompt).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template=PromptTemplate(\n",
    "#     template=\"can you say hello to {name} in 5 different language\",\n",
    "#     input_variables=['name']\n",
    "# )\n",
    "\n",
    "# openai_model.invoke({'name':'sunny'}).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rest of the prompting\n",
    "output parser\n",
    "chaining\n",
    "text splitting\n",
    "vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai_model.invoke({\"name\":\"sunny\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "System_Message\n",
    "HumanMessage\n",
    "AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template=ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\",\"you are a helpful {domain} expert\"),\n",
    "        (\"human\",\"explain the {topic} in simple terms\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = chat_template.invoke({\"domain\":\"medical\",\"topic\":\"maleria\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='you are a helpful medical expert', additional_kwargs={}, response_metadata={}), HumanMessage(content='explain the maleria in simple terms', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[SystemMessage(content='you are a helpful medical expert', additional_kwargs={}, response_metadata={}), HumanMessage(content='explain the maleria in simple terms', additional_kwargs={}, response_metadata={})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Malaria is a disease caused by a parasite that enters the human body through the bite of a certain type of mosquito. These mosquitoes carry the parasite, called Plasmodium, in their saliva and it is passed into your bloodstream when the mosquito bites you.\\n \\nOnce inside your body, the parasite travels to your liver and multiplies. It later enters your bloodstream again and starts infecting your red blood cells. Within 48 to 72 hours, these infected blood cells start to burst and this is where symptoms usually start to show. \\n\\nPeople with malaria often feel very sick and have high fevers, chills, and flu-like illness. It can be dangerous and lead to serious complications if not treated quickly, especially in children and pregnant women. Thankfully, malaria is preventable and curable with the right medication and precautions like using mosquito nets and insect repellents.'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_model.invoke(prompt).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = chat_template.invoke({\"domain\":\"education\",\"topic\":\"AI\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='you are a helpful education expert', additional_kwargs={}, response_metadata={}), HumanMessage(content='explain the AI in simple terms', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure! Malaria is a disease that you can get if a certain type of mosquito, called an Anopheles mosquito, bites you. These mosquitos carry a parasite called Plasmodium, and when they bite you they can transfer this parasite into your blood.\\n\\nOnce the parasite is in your blood, it travels to your liver and starts to multiply. After a while, the new parasites leave your liver and begin to infect and destroy your red blood cells. This can cause a number of symptoms, like high fever, chills, sweats, headache, nausea, and body pain.\\n\\nMalaria can be very serious or even deadly if it's not treated, but it is mostly a problem in tropical and subtropical countries. If you're travelling to an area where malaria is common, there are medicines you can take to help prevent it. If you do get malaria, it can usually be treated with specific drugs. It's also important to try and avoid mosquito bites by using insect repellent and sleeping under a mosquito net.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 209, 'prompt_tokens': 24, 'total_tokens': 233, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d73a38c8-9043-4eb6-829b-c40bb8f0fd63-0', usage_metadata={'input_tokens': 24, 'output_tokens': 209, 'total_tokens': 233, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chaining using LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StrOutputParser()"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "    template=\"can you give me a detail explaination of {topic}\",\n",
    "    input_variables=['topic']\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='can you give me a detail explaination of {topic}')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000162EB87C790>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000162EB87FAC0>, root_client=<openai.OpenAI object at 0x00000162EB832620>, root_async_client=<openai.AsyncOpenAI object at 0x00000162EB87CAF0>, model_name='gpt-4', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple llm chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=prompt | openai_model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Machine learning is a subset of artificial intelligence (AI) that allows systems to automatically learn and improve from experience without being explicitly programmed. This is achieved by feeding data to algorithms, which use statistical techniques to predict outputs. \\n\\nLet's break it down into several key parameters:\\n\\n1. **Algorithms:** These are mathematical/statistical models or specific computations that allow a machine to perform certain tasks. They are essentially the rules or instructions that a machine follows. \\n\\n2. **Data:** This is the information the machine uses to learn. This can be structured data (for example, databases, spreadsheets, etc.) or unstructured data (for example, text, images, audio files, etc.).\\n\\n3. **Training:** This is the process through which a machine learns from data. By feeding data through an algorithm, the machine makes predictions or decisions without being explicitly programmed to perform these tasks. The machine fine-tunes its algorithms to improve the accuracy of its predictions or decisions.\\n\\n4. **Types of Machine Learning**: There are three main types - Supervised Learning where the machine is taught by example, Unsupervised Learning where the machine finds patterns and relationships in the data on its own, and Reinforcement Learning where the machine learns based on reward/punishment feedbacks.\\n\\nExamples of machine learning include email filtering, detection of network intruders, and computer vision, which enables face recognition in images. Machine learning is now an integral part of our daily lives, powering everything from Google's search results to Netflix's movie recommendations.\\n\\nThe objective of machine learning is to develop models that can receive input and predict an output with the use of statistical analysis. It's all about recognizing patterns, making decisions, and producing results. \\n\\nIt's important to note, however, these models are not 100% accurate and can produce errors, but the goal is to minimize these errors as much as possible.\\n\\nIn summary, machine learning is a process of making machines or computers more intelligent by allowing them to learn from data and make predictions or decisions without having to be explicitly programmed for those tasks. It's a rapidly evolving field, making up an increasingly large part of many companies business strategies.\""
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\":\"machine learning\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "      +------------+       \n",
      "      | ChatOpenAI |       \n",
      "      +------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential llm chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1=PromptTemplate(\n",
    "    template=\"get a detail report on {topic}\",\n",
    "    input_variables=[\"topic\"]\n",
    ")\n",
    "\n",
    "prompt2=PromptTemplate(\n",
    "    template=\"generate a 3 point of summary from the following {text}\",\n",
    "    input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"In November 2023, xAI began previewing Grok as a chatbot to selected people,[10] with participation in the early access program being limited to paid X Premium users.[11]\n",
    "\n",
    "It was announced that once the bot was out of early beta, it would only be available to higher tier X Premium+ subscribers.[12]\n",
    "\n",
    "At the time of the preview, xAI described the chatbot as \"a very early beta product  the best we could do with 2 months of training\" that could \"improve rapidly with each passing week\".[13]\n",
    "\n",
    "On March 11, 2024, Musk posted on X that the language model would go open source within a week. Six days later, on March 17, Grok-1 was open sourced under the Apache-2.0 license.[14][15] Disclosed were the networks architecture and its weight parameters.[16]\n",
    "\n",
    "On March 26, 2024, Musk announced that Grok would be enabled for premium subscribers, not just those on the higher-end tier, Premium+.[17]\n",
    "\n",
    "Grok-1.5\n",
    "Grok-1.5\n",
    "Developer(s)\txAI\n",
    "Initial release\tMay 15, 2024; 10 months ago\n",
    "Predecessor\tGrok-1.5\n",
    "Successor\tGrok-2\n",
    "Type\t\n",
    "Large language model\n",
    "Foundation model\n",
    "License\tProprietary\n",
    "Website\tx.ai/blog/grok-1.5\n",
    "On March 29, 2024, Grok-1.5 was announced, with \"improved reasoning capabilities\" and a context length of 128,000 tokens.[18] Grok-1.5 was released to all X Premium users on May 15, 2024.[1]\n",
    "\n",
    "On April 4, 2024, an update to X's \"Explore\" page included summaries of breaking news stories written by Grok, a task previously assigned to a human curation team.[19]\n",
    "\n",
    "On April 12, 2024, Grok-1.5 Vision (Grok-1.5V) was announced. Grok-1.5V is able to process a wide variety of visual information, including documents, diagrams, graphs, screenshots, and photographs.[20] Grok-1.5V was never released to the public.\n",
    "\n",
    "On May 4, 2024, Grok became available in the United Kingdom,[21] that being the only country in Europe to support Grok at the moment due to the impending Artificial Intelligence Act rules in the European Union. Grok was later reviewed by the EU and was released on May 16, 2024.[22]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1=PromptTemplate(\n",
    "    template=\"analysis the the given text carefully {text} and take the necessary data\",\n",
    "    input_variables=[\"topic\"]\n",
    ")\n",
    "\n",
    "prompt2=PromptTemplate(\n",
    "    template=\"summarize the given text in 2 bullet points {text}\",\n",
    "    input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000162EB87C790>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000162EB87FAC0>, root_client=<openai.OpenAI object at 0x00000162EB832620>, root_async_client=<openai.AsyncOpenAI object at 0x00000162EB87CAF0>, model_name='gpt-4', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StrOutputParser()"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt1 | openai_model | parser | prompt2 | openai_model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "      +------------+       \n",
      "      | ChatOpenAI |       \n",
      "      +------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "      +------------+       \n",
      "      | ChatOpenAI |       \n",
      "      +------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=chain.invoke({\"text\":text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- xAI introduced Grok, a chatbot, in November 2023 to select X Premium subscribers, and then to premium subscribers in March 2024. Grok's network architecture, weight parameters, and the language model were open-sourced in March, and an updated version, Grok-1.5, boasting improved reasoning capabilities was unveiled and later released in May 2024 to all X Premium users. \n",
      "- Other developments included Grok writing news story summaries on X's \"Explore\" page in April 2024, and the announcement of Grok-1.5 Vision version with visual information processing abilities which was never released to the public. By May 2024, Grok was launched in the UK and given the green light for release by the EU pending Artificial Intelligence Act rules.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1=PromptTemplate(\n",
    "    template=\"generate simple summary from the following text \\n {text}\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "prompt2=PromptTemplate(\n",
    "    template=\"generate 3 question and answer from the following text \\n {text}\",\n",
    "    input_variables=[\"text\"]\n",
    "    )\n",
    "\n",
    "prompt3=PromptTemplate(\n",
    "    template=\"analysis the summary and qa and generate the 5 important quiz with 4 possible answer \\n summary: {summary}, Q&A: {qa}\",\n",
    "    input_variables=[\"summary\",\"qa\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_chain=RunnableParallel({\n",
    "    \"summary\": prompt1 | openai_model | parser,\n",
    "    \"qa\" : prompt2 | openai_model | parser\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  summary: PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='generate simple summary from the following text \\n {text}')\n",
       "           | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000162EB87C790>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000162EB87FAC0>, root_client=<openai.OpenAI object at 0x00000162EB832620>, root_async_client=<openai.AsyncOpenAI object at 0x00000162EB87CAF0>, model_name='gpt-4', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "           | StrOutputParser(),\n",
       "  qa: PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='generate 3 question and answer from the following text \\n {text}')\n",
       "      | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000162EB87C790>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000162EB87FAC0>, root_client=<openai.OpenAI object at 0x00000162EB832620>, root_async_client=<openai.AsyncOpenAI object at 0x00000162EB87CAF0>, model_name='gpt-4', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "      | StrOutputParser()\n",
       "}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'Intelligent agents are a form of digital agency that actively pursues its goals, makes decisions, and takes actions over time. They vary in complexity, from basic control systems to complex human-like entities. These agents operate based on an objective function that represents their goals and plans actions to maximize its expected value. They are a topic of study in various fields, including artificial intelligence, economics, cognitive science, ethics, and philosophy. They can be described as abstract functional systems and are often closely related to software agents that perform tasks for users.\\n\\nIn other terms, intelligent agents are computer programs designed to independently achieve specific objectives, embodying a new level of digital agency by making proactive, goal-oriented decisions over time. They range from simple control mechanisms like thermostats to advanced, human-like systems. These agents are informed by an objective function representing their goals and work towards maximizing its value. They are not only pivotal in AI but also hold importance in fields like economics, cognitive science, philosophy, and ethics.',\n",
       " 'qa': '1) What is an example of a simple intelligent agent?\\nAnswer: A basic thermostat or control system is considered a simple intelligent agent.\\n\\n2) What encapsulates the goals of an intelligent agent and forms the basis for its operations?\\nAnswer: An objective function encapsulates the goals of an intelligent agent and forms the basis for its operations. \\n\\n3) What are software agents in the field of artificial intelligence and how are they related to intelligent agents?\\nAnswer: Software agents are autonomous computer programs that carry out tasks on behalf of users. They are closely related to intelligent agents, often performing tasks and decision making in a proactive manner.'}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_chain.invoke({\"text\":text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'summary': 'Intelligent agents are a form of digital agency that actively pursues its goals, makes decisions, and takes actions over time. They vary in complexity, from basic control systems to complex human-like entities. These agents operate based on an objective function that represents their goals and plans actions to maximize its expected value. They are a topic of study in various fields, including artificial intelligence, economics, cognitive science, ethics, and philosophy. They can be described as abstract functional systems and are often closely related to software agents that perform tasks for users.\\n\\nIn other terms, intelligent agents are computer programs designed to independently achieve specific objectives, embodying a new level of digital agency by making proactive, goal-oriented decisions over time. They range from simple control mechanisms like thermostats to advanced, human-like systems. These agents are informed by an objective function representing their goals and work towards maximizing its value. They are not only pivotal in AI but also hold importance in fields like economics, cognitive science, philosophy, and ethics.',\n",
    " 'qa': '1) What is an example of a simple intelligent agent?\\nAnswer: A basic thermostat or control system is considered a simple intelligent agent.\\n\\n2) What encapsulates the goals of an intelligent agent and forms the basis for its operations?\\nAnswer: An objective function encapsulates the goals of an intelligent agent and forms the basis for its operations. \\n\\n3) What are software agents in the field of artificial intelligence and how are they related to intelligent agents?\\nAnswer: Software agents are autonomous computer programs that carry out tasks on behalf of users. They are closely related to intelligent agents, often performing tasks and decision making in a proactive manner.'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Intelligent agents, also known as AI agents, are preemptive agents that perform certain actions to achieve predefined goals. They extend the concept of conventional agents by not only responding but also acting preemptively, making decisions, and taking actions over an extended period. These AI agents can range from simple systems like a thermostat to more complex beings like humans or even larger systems such as firms, states, or biomes. \n",
    "\n",
    "The behavior of these intelligent agents is driven by an objective function that encapsulates their targets. They function with the aim of maximizing the expected value of this objective function. Notable examples include a reinforcement learning agent guided by a reward function, and an evolutionary algorithm steered by a fitness function. The reward function for the reinforcement learning agent plays a crucial role by allowing programmers to shape its desired behavior. Similarly, the fitness function acts as the steering wheel for an evolutionary algorithm's behavior. \n",
    "\n",
    "The paradigms of intelligent agents are probed in various fields such as cognitive science, ethics, philosophy, and computer social simulations. These agents are often described in abstract terms, making them highly similar to computer programs. This schematic description often results in them being referred to as abstract intelligent agents or, borrowing a term from economics, \"rational agents\". Also, they have a close relation to software agents, which are autonomous computer programs that carry out tasks on behalf of users. Such software agents are another key application of the concept of intelligent agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_chain= prompt3 | openai_model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = parallel_chain | merge_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            +---------------------------+            \n",
      "            | Parallel<summary,qa>Input |            \n",
      "            +---------------------------+            \n",
      "                 **               **                 \n",
      "              ***                   ***              \n",
      "            **                         **            \n",
      "+----------------+                +----------------+ \n",
      "| PromptTemplate |                | PromptTemplate | \n",
      "+----------------+                +----------------+ \n",
      "          *                               *          \n",
      "          *                               *          \n",
      "          *                               *          \n",
      "  +------------+                    +------------+   \n",
      "  | ChatOpenAI |                    | ChatOpenAI |   \n",
      "  +------------+                    +------------+   \n",
      "          *                               *          \n",
      "          *                               *          \n",
      "          *                               *          \n",
      "+-----------------+              +-----------------+ \n",
      "| StrOutputParser |              | StrOutputParser | \n",
      "+-----------------+              +-----------------+ \n",
      "                 **               **                 \n",
      "                   ***         ***                   \n",
      "                      **     **                      \n",
      "           +----------------------------+            \n",
      "           | Parallel<summary,qa>Output |            \n",
      "           +----------------------------+            \n",
      "                          *                          \n",
      "                          *                          \n",
      "                          *                          \n",
      "                 +----------------+                  \n",
      "                 | PromptTemplate |                  \n",
      "                 +----------------+                  \n",
      "                          *                          \n",
      "                          *                          \n",
      "                          *                          \n",
      "                   +------------+                    \n",
      "                   | ChatOpenAI |                    \n",
      "                   +------------+                    \n",
      "                          *                          \n",
      "                          *                          \n",
      "                          *                          \n",
      "                +-----------------+                  \n",
      "                | StrOutputParser |                  \n",
      "                +-----------------+                  \n",
      "                          *                          \n",
      "                          *                          \n",
      "                          *                          \n",
      "              +-----------------------+              \n",
      "              | StrOutputParserOutput |              \n",
      "              +-----------------------+              \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"AI agent or simply agent), expands this concept by proactively pursuing goals, making decisions, and taking actions over extended periods, thereby exemplifying a novel form of digital agency.[1]\n",
    "\n",
    "Intelligent agents can range from simple to highly complex. A basic thermostat or control system is considered an intelligent agent, as is a human being, or any other system that meets the same criteriasuch as a firm, a state, or a biome.[2]\n",
    "\n",
    "Intelligent agents operate based on an objective function, which encapsulates their goals. They are designed to create and execute plans that maximize the expected value of this function upon completion.[3] For example, a reinforcement learning agent has a reward function, which allows programmers to shape its desired behavior.[4] Similarly, an evolutionary algorithm's behavior is guided by a fitness function.[5]\n",
    "\n",
    "Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\n",
    "\n",
    "Intelligent agents are often described schematically as abstract functional systems similar to computer programs. To distinguish theoretical models from real-world implementations, abstract descriptions of intelligent agents are called abstract intelligent agents. Intelligent agents are also closely related to software agentsautonomous computer programs that carry out tasks on behalf of users. They are also referred to using a term borrowed from economics: a \"rational agent\".[2]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=chain.invoke({\"text\":text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quiz:\n",
      "\n",
      "1) What are Intelligent agents in the field of artificial intelligence?\n",
      "a) They are real-world implementations of theoretical models known as abstract intelligent agents. \n",
      "b) They are autonomous computer programs that carry out tasks on behalf of users.\n",
      "c) They are artificial intelligence entities that actively pursue goals, make decisions, and operate over extended periods.\n",
      "d) They are a concept that proactively pursues goals, makes decisions, and takes actions over extended periods, thereby exemplifying a new form of digital agency.\n",
      "\n",
      "2) How complex can intelligent agents be?\n",
      "a) They can only be as complex as a thermostat.\n",
      "b) They can be as complex as a human being or a firm.\n",
      "c) They can only be as complex as a computer program.\n",
      "d) They can only be as complex as the task they are designed for.\n",
      "\n",
      "3) How do intelligent agents function?\n",
      "a) They function based on an objective function that encapsulates their goals.\n",
      "b) They function based on the level of complexity they are at.\n",
      "c) They function based on the task they are carrying out.\n",
      "d) They function based on the system they are part of.\n",
      "\n",
      "4) What fields beyond artificial intelligence are intelligent agents studied in?\n",
      "a) They are studied only in the field of computer science.\n",
      "b) They are studied in fields such as economics, cognitive science, ethics, and philosophy.\n",
      "c) They are studied in the field of robotics.\n",
      "d) They are studied in the field of data science.\n",
      "\n",
      "5) What does the term \"rational agents\" refer to in the context of artificial intelligence?\n",
      "a) It refers to artificial intelligence entities that actively pursue goals.\n",
      "b) It refers to autonomous computer programs that carry out tasks on behalf of users.\n",
      "c) It refers to intelligent agents who function based on an objective function.\n",
      "d) It refers to the complexity level of intelligent agents.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets understand the parser now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=PromptTemplate(\n",
    "    template=\"generate a prices 3 point summary from given text /n {text}\",\n",
    "    input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = template | openai_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_parser = template | openai_model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. Intelligent agents range in complexity and are found in various forms like basic control systems, human beings, firm, state or biome. They operate based on an objective function to achieve goals, taking decisions and actions over extended periods.\\n2. These agents create plans to maximize their objective function\\'s expected value. They are guided by reward functions in case of a reinforcement learning agent or a fitness function in the case of an evolutionary algorithm.\\n3. Intelligent agents can be abstract or functional systems similar to computer programs. Real-world implementations are often referred to as software agents that perform tasks on behalf of users. An alternative term used for them is \"rational agent\".', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 132, 'prompt_tokens': 317, 'total_tokens': 449, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5665431d-4d5b-4c45-abfe-8b4623f94b03-0', usage_metadata={'input_tokens': 317, 'output_tokens': 132, 'total_tokens': 449, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"text\":text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Intelligent agents in AI proactively pursue goals and perform actions, showcasing a unique form of digital agency. They vary in complexity, from simple thermostats to human beings or even larger systems like firms or states.\\n   \\n2. These agents operate based on an objective function encapsulating their goals. They are created to execute plans that maximize the expected value of their function upon completion. Specific agent types, like reinforcement learning agents and evolutionary algorithms, have particular behaviors guided by reward or fitness functions respectively.\\n\\n3. Intelligent agents are often depicted as abstract functional systems akin to computer programs. Distinctions are made between theoretical models (abstract intelligent agents) and real-world implementations, which are also related to software agents; autonomous programs that perform tasks for users. These are sometimes referred to as \"rational agents.\"'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_parser.invoke({\"text\":text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### json output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=PromptTemplate(\n",
    "    template=\"give me name, age and city from the provided text {text} \\n {format_instructions}\" ,\n",
    "    input_variables=['text'],\n",
    "    partial_variables={\"format_instructions\":parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"hi my name is sunny savita my age is 29 and i am belong to bengaluru\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=template.format(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'give me name, age and city from the provided text hi my name is sunny savita my age is 29 and i am belong to bengaluru \\n Return a JSON object.'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=openai_model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{\\n\"name\": \"sunny savita\",\\n\"age\": 29,\\n\"city\": \"bengaluru\"\\n}', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 45, 'total_tokens': 71, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8ffe92fb-7a92-4eef-9e66-35549d7e53a8-0', usage_metadata={'input_tokens': 45, 'output_tokens': 26, 'total_tokens': 71, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n\"name\": \"sunny savita\",\\n\"age\": 29,\\n\"city\": \"bengaluru\"\\n}'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=parser.parse(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sunny savita'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bengaluru'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"city\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain= template | openai_model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'sunny savita', 'age': 29, 'city': 'bengaluru'}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"text\":text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic=\"\"\"AI agent or simply agent), expands this concept by proactively pursuing goals, making decisions, and taking actions over extended periods, thereby exemplifying a novel form of digital agency.[1]\n",
    "\n",
    "Intelligent agents can range from simple to highly complex. A basic thermostat or control system is considered an intelligent agent, as is a human being, or any other system that meets the same criteriasuch as a firm, a state, or a biome.[2]\n",
    "\n",
    "Intelligent agents operate based on an objective function, which encapsulates their goals. They are designed to create and execute plans that maximize the expected value of this function upon completion.[3] For example, a reinforcement learning agent has a reward function, which allows programmers to shape its desired behavior.[4] Similarly, an evolutionary algorithm's behavior is guided by a fitness function.[5]\n",
    "\n",
    "Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\n",
    "\n",
    "Intelligent agents are often described schematically as abstract functional systems similar to computer programs. To distinguish theoretical models from real-world implementations, abstract descriptions of intelligent agents are called abstract intelligent agents. Intelligent agents are also closely related to software agentsautonomous computer programs that carry out tasks on behalf of users. They are also referred to using a term borrowed from economics: a \"rational agent\".[2]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template2=PromptTemplate(\n",
    "    template='Give me 5 facts about {topic} \\n {format_instruction}',\n",
    "    input_variables=['topic'],\n",
    "    partial_variables={'format_instruction': parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain= template2 | openai_model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Facts': [{'Fact 1': 'Intelligent agents can range from simple to highly complex. Basic systems like a thermostat or more complex entities like a human being or a state can be considered intelligent agents.'},\n",
       "  {'Fact 2': 'Intelligent agents operate based on an objective function that encapsulates their goals. They are designed to create and execute plans to maximize the expected value of this function.'},\n",
       "  {'Fact 3': 'The behavior of Intelligent agents like those using reinforcement learning have a reward function designed to inform their behavior while those using an evolutionary algorithm are guided by a fitness function.'},\n",
       "  {'Fact 4': 'Intelligent agents in artificial intelligence are closely related to agents in economics, and they are also studied in various fields like cognitive science, ethics, the philosophy of practical reason, and several interdisciplinary socio-cognitive modeling and computer social simulations.'},\n",
       "  {'Fact 5': 'Intelligent agents are often described schematically as abstract functional systems similar to computer programs. They can be theoretical models or real-world implementations. They are closely related to software agents, which are autonomous computer programs that carry out tasks on behalf of users.'}]}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": topic})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate a strcuture output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema=[\n",
    "    ResponseSchema(name=\"first_fact\", description=\"first fact about text\"),\n",
    "    ResponseSchema(name=\"second_fact\", description=\"second fact about text\"),\n",
    "    ResponseSchema(name=\"third_fact\", description=\"third fact about text\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=StructuredOutputParser.from_response_schemas(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template3 = PromptTemplate(\n",
    "    template='Give 3 fact about {topic} \\n {format_instruction}',\n",
    "    input_variables=['topic'],\n",
    "    partial_variables={'format_instruction':parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = template3 | openai_model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=chain.invoke({\"topic\":topic})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'first_fact': 'Intelligent agents can range from simple to highly complex, including systems like a basic thermostat or control system, a human being, or larger systems like a firm, a state, or a biome.', 'second_fact': 'Intelligent agents operate based on an objective function which encapsulates their goals, and they are designed to create and execute plans that maximize the expected value of this function upon completion.', 'third_fact': 'Intelligent agents in artificial intelligence are closely related to agents in economics, cognitive science, ethics, philosophy of practical reason, socio-cognitive modeling, and computer social simulations.'}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'first_fact': 'Intelligent agents can range from simple to highly complex, including systems like a basic thermostat or control system, a human being, or larger systems like a firm, a state, or a biome.', 'second_fact': 'Intelligent agents operate based on an objective function which encapsulates their goals, and they are designed to create and execute plans that maximize the expected value of this function upon completion.', 'third_fact': 'Intelligent agents in artificial intelligence are closely related to agents in economics, cognitive science, ethics, philosophy of practical reason, socio-cognitive modeling, and computer social simulations.'}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'first_fact': 'Intelligent agents can range from simple to highly complex, including systems like a basic thermostat or control system, a human being, or larger systems like a firm, a state, or a biome.', 'second_fact': 'Intelligent agents operate based on an objective function which encapsulates their goals, and they are designed to create and execute plans that maximize the expected value of this function upon completion.', 'third_fact': 'Intelligent agents in artificial intelligence are closely related to agents in economics, cognitive science, ethics, philosophy of practical reason, socio-cognitive modeling, and computer social simulations.'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pydantic Output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person(BaseModel):\n",
    "    name:str=Field(description=\"name of person\")\n",
    "    age:int=Field(gt=18,description=\"age of person\")\n",
    "    city:str=Field(description=\"name of the city where the person is located\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=PydanticOutputParser(pydantic_object=Person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"name\": {\"description\": \"name of person\", \"title\": \"Name\", \"type\": \"string\"}, \"age\": {\"description\": \"age of person\", \"exclusiveMinimum\": 18, \"title\": \"Age\", \"type\": \"integer\"}, \"city\": {\"description\": \"name of the city where the person is located\", \"title\": \"City\", \"type\": \"string\"}}, \"required\": [\"name\", \"age\", \"city\"]}\\n```'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = PromptTemplate(\n",
    "    template='Generate the nammme, age and city of a fictional {place} person \\n {format_instruction}',\n",
    "    input_variables=['place'],\n",
    "    partial_variables={'format_instruction':parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = template | openai_model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=chain.invoke({\"place\":\"bengaluru\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(name='Rohit Sharma', age=29, city='Bengaluru')"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rohit Sharma\n"
     ]
    }
   ],
   "source": [
    "print(result.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "print(result.age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@vidiptvashist/building-a-vector-database-from-scratch-in-python-6bd683ba5171\n",
    "https://medium.com/data-and-beyond/vector-databases-a-beginners-guide-b050cbbe9ca0\n",
    "https://nexla.com/ai-infrastructure/vector-databases/\n",
    "https://www.geeksforgeeks.org/what-is-a-vector-database/\n",
    "https://lakefs.io/blog/12-vector-databases-2023/\n",
    "chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectordatabase #textsplitter\n",
    "1. pinecone(cloud vdb),weviate\n",
    "#2. weviate, qurdant,milvus\n",
    "3. mongodb\n",
    "4. faiss, chroma(in memory)#poc\n",
    "5. on disk(persist vectordb on disk)\n",
    "6. aws opensearch\n",
    "\n",
    "embdding, meta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://readmedium.com/en/https:/medium.com/@harsh.vardhan7695/mastering-text-splitting-in-langchain-735313216e01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "# Original Text\n",
    "text = \"\"\"LangChain is a powerful framework for building LLM applications. It helps in connecting language models with data sources, APIs, and other components. One of its key features is text processing, where it allows splitting large documents into smaller chunks. This is useful for vector databases, retrieval-augmented generation (RAG), and long-context handling. Another feature is its integration with various AI tools and libraries. This makes LangChain a great choice for developers working on AI-powered applications.\n",
    "\n",
    "LangChain provides seamless support for working with embeddings and vector databases, allowing efficient document retrieval and query resolution. It also simplifies prompt engineering, making it easier to design structured prompts for LLMs. Developers can use LangChain to build sophisticated AI chatbots, content generators, and even research assistants.\n",
    "\n",
    "The framework supports multiple LLM providers, including OpenAI, Hugging Face, and local models like Llama and Mistral. It offers various chain types, such as sequential and parallel chains, enabling flexible workflow automation. LangChain's integration with tools like Pinecone, ChromaDB, and FAISS ensures fast and scalable search capabilities.\n",
    "\n",
    "With built-in memory components, LangChain allows chatbots and virtual assistants to maintain context over extended interactions. This helps in creating human-like conversations that feel more natural. Furthermore, LangChain supports function calling and API interaction, making it ideal for building AI-powered applications that require external data fetching.\n",
    "\n",
    "The ecosystem also includes template-based prompt handling, making it easier to experiment with different LLM configurations. Developers can fine-tune their models, optimize performance, and integrate LangChain with various cloud-based AI solutions. Whether for research, automation, or enterprise applications, LangChain stands out as a robust and flexible framework for AI development.\n",
    ".\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the splitter\n",
    "splitter = CharacterTextSplitter( \n",
    "    separator=\"\\n\",\n",
    "    chunk_size=200,    \n",
    "    chunk_overlap=50 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 518, which is longer than the specified 200\n",
      "Created a chunk of size 355, which is longer than the specified 200\n",
      "Created a chunk of size 346, which is longer than the specified 200\n",
      "Created a chunk of size 361, which is longer than the specified 200\n",
      "Created a chunk of size 387, which is longer than the specified 200\n"
     ]
    }
   ],
   "source": [
    "# Splitting the text\n",
    "chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LangChain is a powerful framework for building LLM applications. It helps in connecting language models with data sources, APIs, and other components. One of its key features is text processing, where it allows splitting large documents into smaller chunks. This is useful for vector databases, retrieval-augmented generation (RAG), and long-context handling. Another feature is its integration with various AI tools and libraries. This makes LangChain a great choice for developers working on AI-powered applications.',\n",
       " 'LangChain provides seamless support for working with embeddings and vector databases, allowing efficient document retrieval and query resolution. It also simplifies prompt engineering, making it easier to design structured prompts for LLMs. Developers can use LangChain to build sophisticated AI chatbots, content generators, and even research assistants.',\n",
       " \"The framework supports multiple LLM providers, including OpenAI, Hugging Face, and local models like Llama and Mistral. It offers various chain types, such as sequential and parallel chains, enabling flexible workflow automation. LangChain's integration with tools like Pinecone, ChromaDB, and FAISS ensures fast and scalable search capabilities.\",\n",
       " 'With built-in memory components, LangChain allows chatbots and virtual assistants to maintain context over extended interactions. This helps in creating human-like conversations that feel more natural. Furthermore, LangChain supports function calling and API interaction, making it ideal for building AI-powered applications that require external data fetching.',\n",
       " 'The ecosystem also includes template-based prompt handling, making it easier to experiment with different LLM configurations. Developers can fine-tune their models, optimize performance, and integrate LangChain with various cloud-based AI solutions. Whether for research, automation, or enterprise applications, LangChain stands out as a robust and flexible framework for AI development.',\n",
       " '.']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "LangChain is a powerful framework for building LLM applications. It helps in connecting language models with data sources, APIs, and other components. One of its key features is text processing, where it allows splitting large documents into smaller chunks. This is useful for vector databases, retrieval-augmented generation (RAG), and long-context handling. Another feature is its integration with various AI tools and libraries. This makes LangChain a great choice for developers working on AI-powered applications.\n",
      "--------------------------------------------------\n",
      "Chunk 2:\n",
      "LangChain provides seamless support for working with embeddings and vector databases, allowing efficient document retrieval and query resolution. It also simplifies prompt engineering, making it easier to design structured prompts for LLMs. Developers can use LangChain to build sophisticated AI chatbots, content generators, and even research assistants.\n",
      "--------------------------------------------------\n",
      "Chunk 3:\n",
      "The framework supports multiple LLM providers, including OpenAI, Hugging Face, and local models like Llama and Mistral. It offers various chain types, such as sequential and parallel chains, enabling flexible workflow automation. LangChain's integration with tools like Pinecone, ChromaDB, and FAISS ensures fast and scalable search capabilities.\n",
      "--------------------------------------------------\n",
      "Chunk 4:\n",
      "With built-in memory components, LangChain allows chatbots and virtual assistants to maintain context over extended interactions. This helps in creating human-like conversations that feel more natural. Furthermore, LangChain supports function calling and API interaction, making it ideal for building AI-powered applications that require external data fetching.\n",
      "--------------------------------------------------\n",
      "Chunk 5:\n",
      "The ecosystem also includes template-based prompt handling, making it easier to experiment with different LLM configurations. Developers can fine-tune their models, optimize performance, and integrate LangChain with various cloud-based AI solutions. Whether for research, automation, or enterprise applications, LangChain stands out as a robust and flexible framework for AI development.\n",
      "--------------------------------------------------\n",
      "Chunk 6:\n",
      ".\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Printing the chunks\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n{'-'*50}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LangChain is a powerful framework for building LLM applications. It helps in connecting language models with data sources, APIs, and other components. One of its key features is text processing, where it allows splitting large documents into smaller chunks. This is useful for vector databases, retrieval-augmented generation (RAG), and long-context handling. Another feature is its integration with various AI tools and libraries. This makes LangChain a great choice for developers working on AI-powered applications.\\n\\nLangChain provides seamless support for working with embeddings and vector databases, allowing efficient document retrieval and query resolution. It also simplifies prompt engineering, making it easier to design structured prompts for LLMs. Developers can use LangChain to build sophisticated AI chatbots, content generators, and even research assistants.',\n",
       " \"The framework supports multiple LLM providers, including OpenAI, Hugging Face, and local models like Llama and Mistral. It offers various chain types, such as sequential and parallel chains, enabling flexible workflow automation. LangChain's integration with tools like Pinecone, ChromaDB, and FAISS ensures fast and scalable search capabilities.\\n\\nWith built-in memory components, LangChain allows chatbots and virtual assistants to maintain context over extended interactions. This helps in creating human-like conversations that feel more natural. Furthermore, LangChain supports function calling and API interaction, making it ideal for building AI-powered applications that require external data fetching.\",\n",
       " 'The ecosystem also includes template-based prompt handling, making it easier to experiment with different LLM configurations. Developers can fine-tune their models, optimize performance, and integrate LangChain with various cloud-based AI solutions. Whether for research, automation, or enterprise applications, LangChain stands out as a robust and flexible framework for AI development.\\n.']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "splitter = TokenTextSplitter(\n",
    "    encoding_name=\"cl100k_base\",  # OpenAI's encoding\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "LangChain is a powerful framework for building LLM applications. It helps in connecting language models with data sources, APIs, and other components. One of its key features is text processing, where it allows splitting large documents into smaller chunks. This is useful for vector databases, retrieval-augmented generation (RAG), and long-context handling. Another feature is its integration with various AI tools and libraries. This makes LangChain a great choice for developers working on AI-powered applications.\n",
      "\n",
      "LangChain provides seamless support\n",
      "--------------------------------------------------\n",
      "Chunk 2:\n",
      " This makes LangChain a great choice for developers working on AI-powered applications.\n",
      "\n",
      "LangChain provides seamless support for working with embeddings and vector databases, allowing efficient document retrieval and query resolution. It also simplifies prompt engineering, making it easier to design structured prompts for LLMs. Developers can use LangChain to build sophisticated AI chatbots, content generators, and even research assistants.\n",
      "\n",
      "The framework supports multiple LLM providers, including OpenAI, Hugging Face, and local models like Llama and Mistral\n",
      "--------------------------------------------------\n",
      "Chunk 3:\n",
      "LM providers, including OpenAI, Hugging Face, and local models like Llama and Mistral. It offers various chain types, such as sequential and parallel chains, enabling flexible workflow automation. LangChain's integration with tools like Pinecone, ChromaDB, and FAISS ensures fast and scalable search capabilities.\n",
      "\n",
      "With built-in memory components, LangChain allows chatbots and virtual assistants to maintain context over extended interactions. This helps in creating human-like conversations that feel more natural. Furthermore, LangChain\n",
      "--------------------------------------------------\n",
      "Chunk 4:\n",
      " over extended interactions. This helps in creating human-like conversations that feel more natural. Furthermore, LangChain supports function calling and API interaction, making it ideal for building AI-powered applications that require external data fetching.\n",
      "\n",
      "The ecosystem also includes template-based prompt handling, making it easier to experiment with different LLM configurations. Developers can fine-tune their models, optimize performance, and integrate LangChain with various cloud-based AI solutions. Whether for research, automation, or enterprise applications, LangChain stands out as a robust\n",
      "--------------------------------------------------\n",
      "Chunk 5:\n",
      " AI solutions. Whether for research, automation, or enterprise applications, LangChain stands out as a robust and flexible framework for AI development.\n",
      ".\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Printing the chunks\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n{'-'*50}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic_base_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
